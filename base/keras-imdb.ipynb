{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 跟着项目学机器学习--IMDB影评数据情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集导入\n",
    "导入IMDB数据集，并且简单分析数据，以及样本的分布情况。主要从以下几点分析数据：\n",
    "1. vocab 字典的大小\n",
    "2. 训练集和测试集的数据量，以及数据是否等长\n",
    "3. 标记样本的分布情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打印出字典的元素总数： 88584\n"
     ]
    }
   ],
   "source": [
    "# 加载词索引文件\n",
    "# 词索引是一个字典，将每个单词转换为一个正整数\n",
    "# key：word\n",
    "# value：index\n",
    "# imdb.get_word_index()\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "print(\"打印出字典的元素总数：\", len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练集和测试集\n",
    "max_features = 20000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,), (25000,), (25000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看数据维度\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 562, 158)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看每个数据是否等长\n",
    "len(x_train[5]), len(x_train[7]), len(x_train[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "        list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])],\n",
       "       dtype=object),\n",
       " array([1, 0]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看样本数据\n",
    "x_train[:2], y_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练样本的标签分布：\n",
      " 1    12500\n",
      "0    12500\n",
      "dtype: int64\n",
      "测试样本的标签分布：\n",
      " 1    12500\n",
      "0    12500\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 查看标签的分布情况\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"训练样本的标签分布：\\n {pd.Series(y_train).value_counts()}\")\n",
    "print(f\"测试样本的标签分布：\\n {pd.Series(y_test).value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看样本的数据形态\n",
    "由于IMDB数据已经做了word to index， 转为正整数了。我们想查看数据，需要根据字典“翻译”成原始文本。\n",
    "1. 我们需要将word_index 转换为 index_word\n",
    "2. 根据index_word 将样本的转为文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据word_index 得到index_word\n",
    "index_word = {v: k for k, v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数，将制定的样本idx 转为文本（英文需要空格分割）\n",
    "def transform2text(idx: int):\n",
    "    data = x_train[idx]\n",
    "    return \" \".join([index_word[i] for i in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"the was rather is him completely br english send to one dvd for kind way are year type but tired talent of am stories slightest coop on her no was although some has as was garbage che's that to to when it as if is herself br bloodsuckers door simply to picture 25 for he silent to holy dramatically to bigger reason was then does sorry very not reason as it out is herself br film's for with and are of tension 4 of human br english send in could is again outrageous movies episode we could that elements for was nothing laugh has of holy laughing lot not me in perfect and of totally most only dreary 2 one an this an as it is fight harry storyline to action much one out will half this of and setting place movie is guide was fight wonderful have then zombies man sense are as am some br didn't\",\n",
       " 0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform2text(100), y_train[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型\n",
    "这是一个典型的NLP任务，根据序列预测分类。通常处理序列问题都采用RNN（循环神经网络）。当然读者也可以尝试别的算法，如Text-CNN也常常用于文本分类问题。或者更高级的预训练等模型。这些内容在后续的项目中都会涉及到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, None, 50)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, None, 64)          29440     \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,066,689\n",
      "Trainable params: 1,066,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class Net:\n",
    "    def __init__(self):\n",
    "        self.embedd = layers.Embedding(max_features, 50)\n",
    "        self.lstm1 = layers.LSTM(units=64, return_sequences=True, activation='tanh')\n",
    "        self.lstm2 = layers.LSTM(units=64, activation='tanh')\n",
    "        self.dense = layers.Dense(64, activation='tanh')\n",
    "        self.sigmoid = layers.Dense(1, activation='sigmoid')\n",
    "        \n",
    "        self.model: Model = None\n",
    "            \n",
    "    def build_model(self):\n",
    "        x_in = Input(shape=(None,))\n",
    "        x = self.embedd(x_in)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.dense(x)\n",
    "        out = self.sigmoid(x)\n",
    "        return Model(x_in, out)\n",
    "    \n",
    "model = Net().build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理数据集\n",
    "x_train = pad_sequences(x_train, maxlen=200)\n",
    "x_test = pad_sequences(x_test, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 200)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编译模型\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss=keras.losses.BinaryCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 132s 5ms/sample - loss: 0.4676 - accuracy: 0.7564 - val_loss: 0.3494 - val_accuracy: 0.8512\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 127s 5ms/sample - loss: 0.2459 - accuracy: 0.9031 - val_loss: 0.3127 - val_accuracy: 0.8665\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 124s 5ms/sample - loss: 0.1712 - accuracy: 0.9370 - val_loss: 0.3465 - val_accuracy: 0.8574\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 121s 5ms/sample - loss: 0.1240 - accuracy: 0.9575 - val_loss: 0.4230 - val_accuracy: 0.8524\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 121s 5ms/sample - loss: 0.0880 - accuracy: 0.9714 - val_loss: 0.5028 - val_accuracy: 0.8511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f87db10d610>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=32)\n",
    "\n",
    "# Train on 25000 samples, validate on 25000 samples\n",
    "# Epoch 1/20\n",
    "# 25000/25000 [==============================] - 6s 251us/sample - loss: 0.4965 - accuracy: 0.7460 - val_loss: 0.4395 - val_accuracy: 0.7946\n",
    "# Epoch 2/20\n",
    "# 25000/25000 [==============================] - 6s 227us/sample - loss: 0.3138 - accuracy: 0.8690 - val_loss: 0.4258 - val_accuracy: 0.8057\n",
    "# Epoch 3/20\n",
    "# 25000/25000 [==============================] - 6s 232us/sample - loss: 0.1754 - accuracy: 0.9342 - val_loss: 0.6682 - val_accuracy: 0.7586\n",
    "# Epoch 4/20\n",
    "# 25000/25000 [==============================] - 6s 230us/sample - loss: 0.0663 - accuracy: 0.9775 - val_loss: 0.9378 - val_accuracy: 0.7436\n",
    "# Epoch 5/20\n",
    "# 25000/25000 [==============================] - 6s 231us/sample - loss: 0.0177 - accuracy: 0.9941 - val_loss: 1.5410 - val_accuracy: 0.7570\n",
    "# Epoch 6/20\n",
    "# 25000/25000 [==============================] - 6s 229us/sample - loss: 0.0064 - accuracy: 0.9981 - val_loss: 2.0868 - val_accuracy: 0.7553\n",
    "# Epoch 7/20\n",
    "# 25000/25000 [==============================] - 6s 234us/sample - loss: 0.0010 - accuracy: 0.9996 - val_loss: 2.8559 - val_accuracy: 0.7558\n",
    "# Epoch 8/20\n",
    "# 25000/25000 [==============================] - 6s 232us/sample - loss: 8.3974e-04 - accuracy: 0.9998 - val_loss: 3.3243 - val_accuracy: 0.7618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 模型已经过拟合了\n",
    "## 在训练集上达到97%的准确率，而在测试集上只有85%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

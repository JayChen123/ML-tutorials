{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对联模型\n",
    "对联数据集采用https://github.com/wb14123/couplet-dataset\n",
    "\n",
    "模型使用albert的tiny版本进行预训练，UniLM实现seq2seq的模型\n",
    "\n",
    "代码参考苏剑林的博客，并使用苏剑林开源的bert4keras框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载训练数据和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path_in = 'couplet/train/in.txt'\n",
    "train_file_path_out = 'couplet/train/out.txt'\n",
    "\n",
    "test_file_path_in = 'couplet/test/in.txt'\n",
    "test_file_path_out = 'couplet/test/out.txt'\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename) as fd:\n",
    "        return fd.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_in = load_data(train_file_path_in)\n",
    "train_out = load_data(train_file_path_out)\n",
    "test_in = load_data(test_file_path_in)\n",
    "test_out = load_data(test_file_path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['晚 风 摇 树 树 还 挺 ',\n",
       " '愿 景 天 成 无 墨 迹 ',\n",
       " '丹 枫 江 冷 人 初 去 ',\n",
       " '忽 忽 几 晨 昏 ， 离 别 间 之 ， 疾 病 间 之 ， 不 及 终 年 同 静 好 ',\n",
       " '闲 来 野 钓 人 稀 处 ',\n",
       " '毋 人 负 我 ， 毋 我 负 人 ， 柳 下 虽 和 有 介 称 ， 先 生 字 此 ， 可 以 谥 此 ',\n",
       " '投 石 向 天 跟 命 斗 ',\n",
       " '深 院 落 滕 花 ， 石 不 点 头 龙 不 语 ',\n",
       " '不 畏 鸿 门 传 汉 祚 ',\n",
       " '新 居 落 成 创 业 始 ']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_in[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载Albert的Tokenizer和model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# bert_path = 'bert_models/albert_tiny_google_zh_489k/'\n",
    "bert_path = 'bert_models/albert_base_google_zh_additional_36k_steps'\n",
    "\n",
    "config_path = os.path.join(bert_path, 'albert_config.json')\n",
    "checkpoint_path = os.path.join(bert_path, 'albert_model.ckpt')\n",
    "dict_path = os.path.join(bert_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 设置后端的tf.keras\n",
    "# os.environ['TF_KERAS'] = '1'\n",
    "\n",
    "from bert4keras.tokenizers import Tokenizer, load_vocab\n",
    "from bert4keras.models import build_transformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 对于对联项目来说，只需要挑出中文即可，这里作为后续优化\n",
    "token_dict = load_vocab(dict_path=dict_path)\n",
    "tokenizer = Tokenizer(token_dict=token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101,\n",
       "  3241,\n",
       "  7599,\n",
       "  3031,\n",
       "  3409,\n",
       "  3409,\n",
       "  6820,\n",
       "  2923,\n",
       "  102,\n",
       "  3247,\n",
       "  7463,\n",
       "  3883,\n",
       "  5709,\n",
       "  5709,\n",
       "  3291,\n",
       "  5273,\n",
       "  102],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(train_in[0], train_out[0], maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据进行编码\n",
    "from bert4keras.snippets import DataGenerator, AutoRegressiveDecoder, sequence_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoupletDataGenerator(DataGenerator):\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids = [], []\n",
    "        for is_end, sample in self.sample(random):\n",
    "            token_id, segment_id = tokenizer.encode(sample[0], sample[1], maxlen=50)\n",
    "            batch_token_ids.append(token_id)\n",
    "            batch_segment_ids.append(segment_id)\n",
    "            \n",
    "            if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                yield [batch_token_ids, batch_segment_ids], None\n",
    "                batch_token_ids, batch_segment_ids = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss层\n",
    "from bert4keras.layers import Loss\n",
    "from bert4keras.backend import K\n",
    "\n",
    "# class CrossEntropy(Loss):\n",
    "#     def compute_loss(self, inputs, mask=None):\n",
    "#         y_true, y_mask, y_pred = inputs\n",
    "#         y_true = y_true[:, 1:]\n",
    "#         y_mask = y_mask[:, 1:]\n",
    "#         y_pred = y_true[:, :-1]\n",
    "        \n",
    "#         loss = K.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "#         loss = K.sum(loss * y_mask) / K.sum(y_mask)\n",
    "#         return loss\n",
    "        \n",
    "class CrossEntropy(Loss):\n",
    "    \"\"\"交叉熵作为loss，并mask掉输入部分\n",
    "    \"\"\"\n",
    "    def compute_loss(self, inputs, mask=None):\n",
    "        y_true, y_mask, y_pred = inputs\n",
    "        y_true = y_true[:, 1:]  # 目标token_ids\n",
    "        y_mask = y_mask[:, 1:]  # segment_ids，刚好指示了要预测的部分\n",
    "        y_pred = y_pred[:, :-1]  # 预测序列，错开一位\n",
    "        loss = K.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "        loss = K.sum(loss * y_mask) / K.sum(y_mask)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载Albert模型\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = build_transformer_model(config_path=config_path, checkpoint_path=checkpoint_path, model='albert', application='unilm')\n",
    "output = CrossEntropy(2)(model.inputs + model.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (Embedding)     multiple             16226304    Input-Token[0][0]                \n",
      "                                                                 MLM-Norm[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Attention-UniLM-Mask (Lambda)   (None, 1, None, None 0           Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-MultiHeadSelfAttent (None, None, 768)    2362368     Embedding-Norm[0][0]             \n",
      "                                                                 Embedding-Norm[0][0]             \n",
      "                                                                 Embedding-Norm[0][0]             \n",
      "                                                                 Attention-UniLM-Mask[0][0]       \n",
      "                                                                 Transformer-FeedForward-Norm[0][0\n",
      "                                                                 Transformer-FeedForward-Norm[0][0\n",
      "                                                                 Transformer-FeedForward-Norm[0][0\n",
      "                                                                 Attention-UniLM-Mask[0][0]       \n",
      "                                                                 Transformer-FeedForward-Norm[1][0\n",
      "                                                                 Transformer-FeedForward-Norm[1][0\n",
      "                                                                 Transformer-FeedForward-Norm[1][0\n",
      "                                                                 Attention-UniLM-Mask[0][0]       \n",
      "                                                                 Transformer-FeedForward-Norm[2][0\n",
      "                                                                 Transformer-FeedForward-Norm[2][0\n",
      "                                                                 Transformer-FeedForward-Norm[2][0\n",
      "                                                                 Attention-UniLM-Mask[0][0]       \n",
      "                                                                 Transformer-FeedForward-Norm[3][0\n",
      "                                                                 Transformer-FeedForward-Norm[3][0\n",
      "                                                                 Transformer-FeedForward-Norm[3][0\n",
      "                                                                 Attention-UniLM-Mask[0][0]       \n",
      "                                                                 Transformer-FeedForward-Norm[4][0\n",
      "                                                                 Transformer-FeedForward-Norm[4][0\n",
      "                                                                 Transformer-FeedForward-Norm[4][0\n",
      "                                                                 Attention-UniLM-Mask[0][0]       \n",
      "                                                                 Transformer-FeedForward-Norm[5][0\n",
      "                                                                 Transformer-FeedForward-Norm[5][0\n",
      "                                                                 Transformer-FeedForward-Norm[5][0\n",
      "                                                                 Attention-UniLM-Mask[0][0]       \n",
      "                                                                 Transformer-FeedForward-Norm[6][0\n",
      "                                                                 Transformer-FeedForward-Norm[6][0\n",
      "                                                                 Transformer-FeedForward-Norm[6][0\n",
      "                                                                 Attention-UniLM-Mask[0][0]       \n",
      "                                                                 Transformer-FeedForward-Norm[7][0\n",
      "                                                                 Transformer-FeedForward-Norm[7][0\n",
      "                                                                 Transformer-FeedForward-Norm[7][0\n",
      "                                                                 Attention-UniLM-Mask[0][0]       \n",
      "                                                                 Transformer-FeedForward-Norm[8][0\n",
      "                                                                 Transformer-FeedForward-Norm[8][0\n",
      "                                                                 Transformer-FeedForward-Norm[8][0\n",
      "                                                                 Attention-UniLM-Mask[0][0]       \n",
      "                                                                 Transformer-FeedForward-Norm[9][0\n",
      "                                                                 Transformer-FeedForward-Norm[9][0\n",
      "                                                                 Transformer-FeedForward-Norm[9][0\n",
      "                                                                 Attention-UniLM-Mask[0][0]       \n",
      "                                                                 Transformer-FeedForward-Norm[10][\n",
      "                                                                 Transformer-FeedForward-Norm[10][\n",
      "                                                                 Transformer-FeedForward-Norm[10][\n",
      "                                                                 Attention-UniLM-Mask[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-MultiHeadSelfAttent (None, None, 768)    0           Embedding-Norm[0][0]             \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[0][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[1][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[2][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[3][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[4][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[5][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[6][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[7][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[8][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[9][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[10][\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-MultiHeadSelfAttent (None, None, 768)    1536        Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-FeedForward (FeedFo (None, None, 768)    4722432     Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-FeedForward-Add (Ad (None, None, 768)    0           Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[0][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[1][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[2][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[3][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[4][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[5][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[6][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[7][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[8][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[9][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[10][0]   \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[11][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-FeedForward-Norm (L (None, None, 768)    1536        Transformer-FeedForward-Add[0][0]\n",
      "                                                                 Transformer-FeedForward-Add[1][0]\n",
      "                                                                 Transformer-FeedForward-Add[2][0]\n",
      "                                                                 Transformer-FeedForward-Add[3][0]\n",
      "                                                                 Transformer-FeedForward-Add[4][0]\n",
      "                                                                 Transformer-FeedForward-Add[5][0]\n",
      "                                                                 Transformer-FeedForward-Add[6][0]\n",
      "                                                                 Transformer-FeedForward-Add[7][0]\n",
      "                                                                 Transformer-FeedForward-Add[8][0]\n",
      "                                                                 Transformer-FeedForward-Add[9][0]\n",
      "                                                                 Transformer-FeedForward-Add[10][0\n",
      "                                                                 Transformer-FeedForward-Add[11][0\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Dense (Dense)               (None, None, 768)    590592      Transformer-FeedForward-Norm[11][\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Norm (LayerNormalization)   (None, None, 768)    1536        MLM-Dense[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "MLM-Bias (BiasAdd)              (None, None, 21128)  21128       Embedding-Token[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "MLM-Activation (Activation)     (None, None, 21128)  0           MLM-Bias[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "cross_entropy_1 (CrossEntropy)  (None, None, 21128)  0           Input-Token[0][0]                \n",
      "                                                                 Input-Segment[0][0]              \n",
      "                                                                 MLM-Activation[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 24,323,720\n",
      "Trainable params: 24,323,720\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinct/anaconda3/lib/python3.8/site-packages/keras/engine/training_utils.py:816: UserWarning: Output cross_entropy_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to cross_entropy_1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Model(model.inputs, output)\n",
    "model.compile(optimizer=Adam(1e-5))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动对下联\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AutoNextCouplet(AutoRegressiveDecoder):\n",
    "    \n",
    "    @AutoRegressiveDecoder.wraps(default_rtype='probas')\n",
    "    def predict(self, inputs, output_ids, stats):\n",
    "        token_id, segment_id = inputs\n",
    "        token_id = np.concatenate([token_id, output_ids], 1)\n",
    "        segment_id = np.concatenate([segment_id, np.ones_like(output_ids)], 1)\n",
    "        return self.last_token(model).predict([token_id, segment_id])\n",
    "    \n",
    "    def next_couplet(self, text, topk=1):\n",
    "        max_len = 50 # albert 最大长度是312 输入和输出均限制150长度\n",
    "        token_id, segment_id = tokenizer.encode(text, maxlen=max_len)\n",
    "        \n",
    "        output_id = self.beam_search([token_id, segment_id], topk=topk)\n",
    "        return tokenizer.decode(output_id)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "next_couplet = AutoNextCouplet(start_id=None, end_id=tokenizer._token_end_id, maxlen=50)\n",
    "\n",
    "class EvalCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.lowest = 1e8\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs['loss'] < self.lowest:\n",
    "            logs['loss'] = self.lowest\n",
    "            model.save_weights('weights/couplet-albert-base-best-weights.weights')\n",
    "        \n",
    "        self.just_show()\n",
    "        \n",
    "    def just_show(self):\n",
    "        first = ['今日天气多云多美丽', '珍藏惟有诗三卷', '狂笔一挥天地动', '推窗问月诗何在']\n",
    "        for each in first:\n",
    "            print(\" -\", each)\n",
    "            print(\"--\", next_couplet.next_couplet(each))\n",
    "            print()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今朝风光万象大风和'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_couplet.next_couplet('今日天气多云多美丽')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CoupletDataGenerator(zip(train_in, train_out))\n",
    "# test_data = CoupletDataGenerator(zip(test_in, test_out))\n",
    "\n",
    "model.fit(train_data.forfit(),epochs=5, steps_per_epoch=10000, callbacks=[EvalCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 今日天气多云多美丽\n",
      "-- 今朝风光万象大风和\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 珍藏不无酒一壶\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 高歌万载日月长\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 对月吟诗酒自酣\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model.load_weights('couplet-best-weights.weights')\n",
    "# EvalCallback().just_show()\n",
    "\n",
    "# albert tiny\n",
    "# - 今日天气多云多美丽\n",
    "# -- 今朝人人有意有情情\n",
    "\n",
    "#  - 珍藏惟有诗三卷\n",
    "# -- 喜见常知画一书\n",
    "\n",
    "#  - 狂笔一挥天地动\n",
    "# -- 新风再绘古今新\n",
    "\n",
    "#  - 推窗问月诗何在\n",
    "# -- 对月吟诗画不同\n",
    "\n",
    "# # albert base\n",
    "# - 今日天气多云多美丽\n",
    "# -- 今朝风光万象大风和\n",
    "\n",
    "#  - 珍藏惟有诗三卷\n",
    "# -- 珍藏不无酒一壶\n",
    "\n",
    "#  - 狂笔一挥天地动\n",
    "# -- 高歌万载日月长\n",
    "\n",
    "#  - 推窗问月诗何在\n",
    "# -- 对月吟诗酒自酣\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'明月照青山'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_couplet.next_couplet('清 风 凝 白 雪 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

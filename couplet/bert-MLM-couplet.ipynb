{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLM实现对联模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_in = \"couplet/train/in.txt\"\n",
    "train_out = \"couplet/train/out.txt\"\n",
    "test_in = \"couplet/test/in.txt\"\n",
    "test_out = \"couplet/test/out.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define load data func\n",
    "from typing import List\n",
    "\n",
    "def load_data(filename: str) -> List[str]:\n",
    "    with open(filename) as fd:\n",
    "        return fd.read().split('\\n')\n",
    "    \n",
    "train_in = load_data(train_in)\n",
    "train_out = load_data(train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer\n",
    "import os\n",
    "from bert4keras.tokenizers import Tokenizer, load_vocab\n",
    "\n",
    "config_path = 'bert_models/albert_base_google_zh_additional_36k_steps/albert_config.json'\n",
    "check_point_path = 'bert_models/albert_base_google_zh_additional_36k_steps/albert_model.ckpt'\n",
    "vocab_path = 'bert_models/albert_base_google_zh_additional_36k_steps/vocab.txt'\n",
    "\n",
    "token_dict = load_vocab(vocab_path)\n",
    "tokenizer = Tokenizer(token_dict=token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert4keras.snippets import DataGenerator, sequence_padding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "MAXLEN = 50 # 编码的最大长度\n",
    "\n",
    "class CoupletData(DataGenerator):\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids, batch_label = [], [], []\n",
    "        for is_end, data in self.sample(random=random):\n",
    "            x, y = data\n",
    "            token_id, segment_id = tokenizer.encode(x, maxlen=MAXLEN)\n",
    "            token_id_label, _ = tokenizer.encode(y, maxlen=MAXLEN)\n",
    "            \n",
    "            batch_token_ids.append(token_id)\n",
    "            batch_segment_ids.append(segment_id)\n",
    "            batch_label.append(token_id_label)\n",
    "            \n",
    "            if len(batch_segment_ids) == self.batch_size or is_end:\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                batch_label = sequence_padding(batch_label)\n",
    "                yield [batch_token_ids, batch_segment_ids], to_categorical(batch_label, num_classes=len(token_dict))\n",
    "                batch_token_ids, batch_segment_ids, batch_label = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['晚 风 摇 树 树 还 挺 ',\n",
       " '愿 景 天 成 无 墨 迹 ',\n",
       " '丹 枫 江 冷 人 初 去 ',\n",
       " '忽 忽 几 晨 昏 ， 离 别 间 之 ， 疾 病 间 之 ， 不 及 终 年 同 静 好 ',\n",
       " '闲 来 野 钓 人 稀 处 ',\n",
       " '毋 人 负 我 ， 毋 我 负 人 ， 柳 下 虽 和 有 介 称 ， 先 生 字 此 ， 可 以 谥 此 ',\n",
       " '投 石 向 天 跟 命 斗 ',\n",
       " '深 院 落 滕 花 ， 石 不 点 头 龙 不 语 ',\n",
       " '不 畏 鸿 门 传 汉 祚 ',\n",
       " '新 居 落 成 创 业 始 ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_in[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 29)\n",
      "(32, 29)\n",
      "(32, 29, 21128)\n",
      "(32, 22)\n",
      "(32, 22)\n",
      "(32, 22, 21128)\n",
      "(32, 25)\n",
      "(32, 25)\n",
      "(32, 25, 21128)\n",
      "(4, 18)\n",
      "(4, 18)\n",
      "(4, 18, 21128)\n"
     ]
    }
   ],
   "source": [
    "for data in CoupletData(zip(train_in[:100], train_out[:100]), batch_size=32):\n",
    "    (token_id, segment_id), label = data\n",
    "    print(token_id.shape)\n",
    "    print(segment_id.shape)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert4keras.models import build_transformer_model\n",
    "\n",
    "model = build_transformer_model(config_path=config_path, checkpoint_path=check_point_path, model='albert', with_mlm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (Embedding)     multiple             16226304    Input-Token[0][0]                \n",
      "                                                                 MLM-Norm[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-MultiHeadSelfAttent (None, None, 768)    2362368     Embedding-Norm[0][0]             \n",
      "                                                                 Embedding-Norm[0][0]             \n",
      "                                                                 Embedding-Norm[0][0]             \n",
      "                                                                 Transformer-FeedForward-Norm[0][0\n",
      "                                                                 Transformer-FeedForward-Norm[0][0\n",
      "                                                                 Transformer-FeedForward-Norm[0][0\n",
      "                                                                 Transformer-FeedForward-Norm[1][0\n",
      "                                                                 Transformer-FeedForward-Norm[1][0\n",
      "                                                                 Transformer-FeedForward-Norm[1][0\n",
      "                                                                 Transformer-FeedForward-Norm[2][0\n",
      "                                                                 Transformer-FeedForward-Norm[2][0\n",
      "                                                                 Transformer-FeedForward-Norm[2][0\n",
      "                                                                 Transformer-FeedForward-Norm[3][0\n",
      "                                                                 Transformer-FeedForward-Norm[3][0\n",
      "                                                                 Transformer-FeedForward-Norm[3][0\n",
      "                                                                 Transformer-FeedForward-Norm[4][0\n",
      "                                                                 Transformer-FeedForward-Norm[4][0\n",
      "                                                                 Transformer-FeedForward-Norm[4][0\n",
      "                                                                 Transformer-FeedForward-Norm[5][0\n",
      "                                                                 Transformer-FeedForward-Norm[5][0\n",
      "                                                                 Transformer-FeedForward-Norm[5][0\n",
      "                                                                 Transformer-FeedForward-Norm[6][0\n",
      "                                                                 Transformer-FeedForward-Norm[6][0\n",
      "                                                                 Transformer-FeedForward-Norm[6][0\n",
      "                                                                 Transformer-FeedForward-Norm[7][0\n",
      "                                                                 Transformer-FeedForward-Norm[7][0\n",
      "                                                                 Transformer-FeedForward-Norm[7][0\n",
      "                                                                 Transformer-FeedForward-Norm[8][0\n",
      "                                                                 Transformer-FeedForward-Norm[8][0\n",
      "                                                                 Transformer-FeedForward-Norm[8][0\n",
      "                                                                 Transformer-FeedForward-Norm[9][0\n",
      "                                                                 Transformer-FeedForward-Norm[9][0\n",
      "                                                                 Transformer-FeedForward-Norm[9][0\n",
      "                                                                 Transformer-FeedForward-Norm[10][\n",
      "                                                                 Transformer-FeedForward-Norm[10][\n",
      "                                                                 Transformer-FeedForward-Norm[10][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-MultiHeadSelfAttent (None, None, 768)    0           Embedding-Norm[0][0]             \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[0][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[1][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[2][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[3][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[4][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[5][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[6][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[7][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[8][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[9][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[10][\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-MultiHeadSelfAttent (None, None, 768)    1536        Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-FeedForward (FeedFo (None, None, 768)    4722432     Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-FeedForward-Add (Ad (None, None, 768)    0           Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[0][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[1][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[2][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[3][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[4][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[5][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[6][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[7][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[8][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[9][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[10][0]   \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[11][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-FeedForward-Norm (L (None, None, 768)    1536        Transformer-FeedForward-Add[0][0]\n",
      "                                                                 Transformer-FeedForward-Add[1][0]\n",
      "                                                                 Transformer-FeedForward-Add[2][0]\n",
      "                                                                 Transformer-FeedForward-Add[3][0]\n",
      "                                                                 Transformer-FeedForward-Add[4][0]\n",
      "                                                                 Transformer-FeedForward-Add[5][0]\n",
      "                                                                 Transformer-FeedForward-Add[6][0]\n",
      "                                                                 Transformer-FeedForward-Add[7][0]\n",
      "                                                                 Transformer-FeedForward-Add[8][0]\n",
      "                                                                 Transformer-FeedForward-Add[9][0]\n",
      "                                                                 Transformer-FeedForward-Add[10][0\n",
      "                                                                 Transformer-FeedForward-Add[11][0\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Dense (Dense)               (None, None, 768)    590592      Transformer-FeedForward-Norm[11][\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Norm (LayerNormalization)   (None, None, 768)    1536        MLM-Dense[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "MLM-Bias (BiasAdd)              (None, None, 21128)  21128       Embedding-Token[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "MLM-Activation (Activation)     (None, None, 21128)  0           MLM-Bias[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 24,323,720\n",
      "Trainable params: 24,323,720\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "from bert4keras.snippets import to_array\n",
    "\n",
    "def next_couplet(text: str):\n",
    "    \"\"\"对下联接口\"\"\"\n",
    "    token_id, segment_id = tokenizer.encode(text, maxlen=50)\n",
    "    token_id, segment_id = to_array([token_id], [segment_id])\n",
    "    y_pred = model.predict([token_id, segment_id])[0]\n",
    "    return tokenizer.decode(y_pred.argmax(-1))\n",
    "\n",
    "class EvalCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.lowest = 1e8\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs['loss'] < self.lowest:\n",
    "            logs['loss'] = self.lowest\n",
    "            model.save_weights('weights/couplet-albert-mlm-best.weights')\n",
    "        \n",
    "        self.just_show()\n",
    "        \n",
    "    def just_show(self):\n",
    "        first = ['今日天气多云多美丽', \n",
    "                 '珍藏惟有诗三卷', \n",
    "                 '狂笔一挥天地动', \n",
    "                 '推窗问月诗何在',\n",
    "                 '彩屏如画，望秀美崤函，花团锦簇']\n",
    "        \n",
    "        for each in first:\n",
    "            print(\" -\", each)\n",
    "            print(\"--\", next_couplet(each))\n",
    "            print()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss=CategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CoupletData(data=zip(train_in, train_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 172s 172ms/step - loss: 1.9731\n",
      " - 今日天气多云多美丽\n",
      "-- 今年春光有月有和谐\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 珍藏不无酒一分\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 大心百载古今行\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 倚月吟风梦自来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 春水如诗，看和明画业，画韵辉流\n",
      "\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 171s 171ms/step - loss: 1.9736\n",
      " - 今日天气多云多美丽\n",
      "-- 今年人光有月有和谐\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 喜乐不为酒一行\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 清风千载古今行\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 对酒吟风梦自来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 春韵似诗，看和谐画业，画韵辉流\n",
      "\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 172s 172ms/step - loss: 1.9705\n",
      " - 今日天气多云多美丽\n",
      "-- 今年风光有月有和明\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 不藏不无酒一行\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 大心三载世今行\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 入月吟风梦自来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 春韵如云，看和明画业，画韵辉流\n",
      "\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 172s 172ms/step - loss: 1.9716\n",
      " - 今日天气多云多美丽\n",
      "-- 今年人风有月有和谐\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 雅雅不为酒一杯\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 狂风三载日今欢\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 对酒吟花酒自来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 彩月如诗，看和明画月，月韵春流\n",
      "\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 172s 172ms/step - loss: 1.9554\n",
      " - 今日天气多云多美丽\n",
      "-- 今年人光有月有和谐\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 苦醉不无酒一杯\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 大心千步古今飞\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 入笔吟风酒自来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 彩笔如诗，看和明大业，花舞春流\n",
      "\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 172s 172ms/step - loss: 1.9412\n",
      " - 今日天气多云多美丽\n",
      "-- 今年人光有月少和谐\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 珍乐当为酒一杯\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 清心千载古今新\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 对笔吟花梦自来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 彩韵如诗，看和谐文业，气韵辉流\n",
      "\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 172s 172ms/step - loss: 1.9500\n",
      " - 今日天气多云多美丽\n",
      "-- 今朝人光有月尽和明\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 喜乐当无酒一杯\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 高心千卷古今流\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 入月观风梦自来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 彩月如云，看和谐华业，日韵人流\n",
      "\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 172s 172ms/step - loss: 1.9397\n",
      " - 今日天气多云多美丽\n",
      "-- 今年人风有月尽和明\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 雅雅不无酒一杯\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 小心千载古今新\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 把月吟风酒自来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 彩韵如诗，看和明华业，日韵春流\n",
      "\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 172s 172ms/step - loss: 1.9281\n",
      " - 今日天气多云多美丽\n",
      "-- 今朝人光有月有和谐\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 苦乐当为酒一行\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 大心三卷古今欢\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 把酒吟风梦自来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 春水如诗，看和明画业，日韵春流\n",
      "\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 172s 172ms/step - loss: 1.9266\n",
      " - 今日天气多云多美丽\n",
      "-- 今年人光有月少和谐\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 苦乐不无酒一行\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 清心三载古今行\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 对酒吟诗酒自来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 彩韵似诗，看和谐画业，月舞人流\n",
      "\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 172s 172ms/step - loss: 1.9297\n",
      " - 今日天气多云多美丽\n",
      "-- 今年人光有月尽和明\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 苦醉不无酒一年\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 清风三载古今行\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 对酒观风月自来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 彩阁似诗，看和明画地，花韵春流\n",
      "\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 172s 172ms/step - loss: 1.9293\n",
      " - 今日天气多云多美丽\n",
      "-- 今年人光有月不和祥\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 苦醉不无酒一杯\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 清风三卷古今飞\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 入酒吟风酒自来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 玉阁似诗，看和谐地，月舞云香\n",
      "\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 172s 172ms/step - loss: 1.9186\n",
      " - 今日天气多云多美丽\n",
      "-- 今朝人光有月更和煌\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 苦醉不无酒一行\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 清心三卷古今流\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 把案吟风梦自来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 彩阁似诗，看和煌盛地，花舞花流\n",
      "\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 171s 171ms/step - loss: 1.9228\n",
      " - 今日天气多云多美丽\n",
      "-- 今朝人光有月更和谐\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 苦醉还无酒一行\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 高风三载古今生\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 把酒吟风酒不来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 彩地如诗，看和谐盛业，气舞人腾\n",
      "\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 173s 173ms/step - loss: 1.9111\n",
      " - 今日天气多云多美丽\n",
      "-- 今年人光有月更清谐\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 苦乐还无酒一行\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 清心三载古今飞\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 把酒吟风酒不来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 彩水似诗，喜和谐景苑，月舞花流\n",
      "\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 171s 171ms/step - loss: 1.9069\n",
      " - 今日天气多云多美丽\n",
      "-- 今年人光有地更和谐\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 苦醉还无酒一杯\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 清心再卷古今飞\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 把酒吟风酒不来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 大地似诗，看和谐社地，花舞春腾\n",
      "\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 172s 172ms/step - loss: 1.9135\n",
      " - 今日天气多云多美丽\n",
      "-- 今朝人光有月尽风明\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 苦醉当无酒一杯\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 清风三卷古今流\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 把酒观风酒不来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 彩韵如诗，看和谐天苑，鸟舞龙流\n",
      "\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 172s 172ms/step - loss: 1.8990\n",
      " - 今日天气多云多美丽\n",
      "-- 今朝人光有水更和谐\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 雅读不无酒一杯\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 高风三卷古今流\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 对案观风酒不来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 玉阁似诗，看和谐华苑，水舞莺流\n",
      "\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.902 - 173s 173ms/step - loss: 1.9023\n",
      " - 今日天气多云多美丽\n",
      "-- 今年风风有月更清明\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 苦乐还无酒一杯\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 清风三唱古今飞\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 把酒吟风酒不来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 彩阁似诗，看和谐社业，水舞花飞\n",
      "\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 172s 172ms/step - loss: 1.9001\n",
      " - 今日天气多云多美丽\n",
      "-- 今朝人光万月更和谐\n",
      "\n",
      " - 珍藏惟有诗三卷\n",
      "-- 喜乐不无酒一杯\n",
      "\n",
      " - 狂笔一挥天地动\n",
      "-- 清风三载古今生\n",
      "\n",
      " - 推窗问月诗何在\n",
      "-- 把案观风酒不来\n",
      "\n",
      " - 彩屏如画，望秀美崤函，花团锦簇\n",
      "-- 彩气似春，看和谐华业，鸟舞龙腾\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f048dbc3f70>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data.forfit(), epochs=20, steps_per_epoch=1000, callbacks=[EvalCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.97 ms ± 403 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit next_couplet('彩屏如画，望秀美崤函，花团锦簇')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.42 ms, sys: 29 ms, total: 38.5 ms\n",
      "Wall time: 37.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'彩气似春，看和谐华业，鸟舞龙腾'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time next_couplet('彩屏如画，望秀美崤函，花团锦簇')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

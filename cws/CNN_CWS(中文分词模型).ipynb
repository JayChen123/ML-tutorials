{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "CNN-CWS(中文分词模型).ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcipIk61Gjvu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3f41ef0-07eb-46f6-be3b-d34d6bb414e4"
      },
      "source": [
        "# 安装模块\n",
        "!pip install tensorflow==2.2.0 bert4keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/1a/0d79814736cfecc825ab8094b39648cc9c46af7af1bae839928acb73b4dd/tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2MB 33kB/s \n",
            "\u001b[?25hCollecting bert4keras\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/39/4cbf03e4cc7ab87beba6a092ce830e297cd0f60bf2c5099ebf964a3b25db/bert4keras-0.10.1.tar.gz (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.19.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.12.4)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.3.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.32.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.15.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (2.10.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.4.1)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 47.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.36.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 52.2MB/s \n",
            "\u001b[?25hCollecting keras<=2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 58.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow==2.2.0) (54.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.27.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras<=2.3.1->bert4keras) (3.13)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
            "Building wheels for collected packages: bert4keras\n",
            "  Building wheel for bert4keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert4keras: filename=bert4keras-0.10.1-cp37-none-any.whl size=44291 sha256=7b184b71ed9b7fe6f5d58e77f9a66efd2568f59a5d89c582ebf2c4e3992ac3cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/b5/7f/7ec0ca4d92aed5a17703bad3f98979d1fda2a924e1a94188f6\n",
            "Successfully built bert4keras\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow, keras-applications, keras, bert4keras\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed bert4keras-0.10.1 keras-2.3.1 keras-applications-1.0.8 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVi3zgrxHo8l"
      },
      "source": [
        "## 读取训练数据\n",
        "数据存放在个人drive硬盘中。需要加载drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q13iHaZmH_pD",
        "outputId": "b2231da0-d8d5-4372-bbee-c0489bc525af"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Heddj3iLG-9N"
      },
      "source": [
        "from typing import List, Dict\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import math\n",
        "\n",
        "train_data_path = '/content/drive/MyDrive/icwb2-data/training/msr_training.utf8'\n",
        "\n",
        "\n",
        "def read_data_from_file(filename: str, encoding='utf-8') -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    原始数据格式如下：\n",
        "    “  这  首先  是  个  民族  问题  ，  民族  的  感情  问题  。\n",
        "    ’  我  扔  了  两颗  手榴弹  ，  他  一下子  出  溜  下去  。\n",
        "    “  废除  先前  存在  的  所有制  关系  ，  并不是  共产主义  所  独具  的  特征  。\n",
        "    读取一行，再对每一行进行空格分割处理，最终返回如下\n",
        "    [\n",
        "        ['这','首先',...],\n",
        "        ['我','扔',...],\n",
        "        ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    with open(filename, 'r', encoding=encoding) as fd:\n",
        "        for line in fd.readlines():\n",
        "            line = line.strip().split(' ')\n",
        "            res.append([w for w in line if w])\n",
        "    return res"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj6iiF0qKOQv"
      },
      "source": [
        "## 创建Tokenizer\n",
        "直接对数据进行编码。包装在List中，方便后续采用batch调用。\n",
        "\n",
        "标签采用4标签原则，分别是\"B、M、E、S\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSje9Zt-Gjvz"
      },
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, data: List[List[str]], tag2id: Dict[str, str]):\n",
        "        self.src_data = data\n",
        "        self.tag2id = tag2id\n",
        "        self.char_data: List[List[int]] = []\n",
        "        self.label: List[List[str]] = []\n",
        "        self.token2id: Dict[str, int] = {}\n",
        "\n",
        "    def tokenize(self):\n",
        "        for sentence in tqdm(self.src_data):\n",
        "            sub_label: List[str] = []\n",
        "            sub_sent: List[str] = []\n",
        "            for token in sentence:\n",
        "                if len(token) == 1:\n",
        "                    sub_sent.append(token)\n",
        "                    sub_label.append(self.tag2id.get('S'))\n",
        "                elif len(token) == 2:\n",
        "                    sub_sent.extend(list(token))\n",
        "                    sub_label.extend([self.tag2id.get('B'), self.tag2id.get('E')])\n",
        "                else:\n",
        "                    sub_sent.extend(list(token))\n",
        "                    sub_label.extend([self.tag2id.get('B')] +\n",
        "                                     [self.tag2id.get('M')] * (len(token) - 2) +\n",
        "                                     [self.tag2id.get('E')])\n",
        "\n",
        "            sub_sent_ = self._token2id(sub_sent)\n",
        "\n",
        "            assert len(sub_sent_) == len(sub_label)\n",
        "            self.char_data.append(sub_sent_)\n",
        "            self.label.append(sub_label)\n",
        "\n",
        "    def _token2id(self, sentence: List[str]) -> List[int]:\n",
        "        res = []\n",
        "        for w in sentence:\n",
        "            if w not in self.token2id:\n",
        "                self.token2id[w] = len(self.token2id) + 1\n",
        "            res.append(self.token2id.get(w))\n",
        "        return res\n",
        "\n",
        "    @property\n",
        "    def id2token(self):\n",
        "        return {v: k for k, v in self.token2id.items()}\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23aX-lamGjv0"
      },
      "source": [
        "class DataLoader(Sequence):\n",
        "    def __init__(self, data, target, batch_size=64):\n",
        "        self.data = data\n",
        "        self.target = target\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_x = self.data[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        batch_y = self.target[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        return pad_sequences(batch_x, padding='post'), to_categorical(pad_sequences(batch_y, padding='post', value=4), num_classes=5)\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.data) / self.batch_size)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyYfuVT1Gjv0",
        "outputId": "e785a908-7279-4d18-f97d-c88013b9231a"
      },
      "source": [
        "pos_tag = list('BMES')\n",
        "\n",
        "tag2id = {item: i for i, item in enumerate(pos_tag)}\n",
        "id2tag = {v: k for k, v in tag2id.items()}\n",
        "print(tag2id)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B': 0, 'M': 1, 'E': 2, 'S': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NoDtP4WGjv0",
        "outputId": "ce9191cc-d4db-44a9-a916-979bd93f4bd1"
      },
      "source": [
        "train_data = read_data_from_file(train_data_path)\n",
        "tokenizer = Tokenizer(train_data, tag2id=tag2id)\n",
        "tokenizer.tokenize()\n",
        "\n",
        "\n",
        "for idx, batch in enumerate(DataLoader(tokenizer.char_data, tokenizer.label)):\n",
        "    x, y = batch\n",
        "    print(x.shape)\n",
        "    print(y.shape)\n",
        "\n",
        "    if idx == 10:\n",
        "        break"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 86924/86924 [00:03<00:00, 28645.46it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(64, 78)\n",
            "(64, 78, 5)\n",
            "(64, 77)\n",
            "(64, 77, 5)\n",
            "(64, 84)\n",
            "(64, 84, 5)\n",
            "(64, 88)\n",
            "(64, 88, 5)\n",
            "(64, 85)\n",
            "(64, 85, 5)\n",
            "(64, 93)\n",
            "(64, 93, 5)\n",
            "(64, 112)\n",
            "(64, 112, 5)\n",
            "(64, 73)\n",
            "(64, 73, 5)\n",
            "(64, 70)\n",
            "(64, 70, 5)\n",
            "(64, 87)\n",
            "(64, 87, 5)\n",
            "(64, 58)\n",
            "(64, 58, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aspqbpCyGjv1"
      },
      "source": [
        "import os\n",
        "# 使用bert4keras模块中的CRF和维特比解码\n",
        "# 需要设置后端，这里使用TF后端\n",
        "\n",
        "os.environ['TF_KERAS'] = '1'\n",
        "\n",
        "from bert4keras.layers import ConditionalRandomField\n",
        "from bert4keras.snippets import ViterbiDecoder"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1S2hd1vK3hP"
      },
      "source": [
        "## 构建模型\n",
        "这里使用简单的CNN+CRF网络。具体参考[苏剑林的案例](https://github.com/bojone/crf/blob/master/word_seg.py)。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFPBlYp0Gjv1",
        "outputId": "324bcadf-1269-4d7b-bd79-07bd7adbb7e1"
      },
      "source": [
        "from tensorflow.keras import Input, Model, Sequential\n",
        "from tensorflow.keras import layers as L\n",
        "\n",
        "CRF = ConditionalRandomField()\n",
        "\n",
        "vocab_size = len(tokenizer.token2id)\n",
        "\n",
        "cnn = Sequential([\n",
        "        L.Embedding(vocab_size + 1, 100),\n",
        "        L.Conv1D(filters=256, kernel_size=3, padding='same', activation='relu'),\n",
        "        L.Conv1D(filters=256, kernel_size=3, padding='same', activation='relu'),\n",
        "        L.Conv1D(filters=256, kernel_size=3, padding='same', activation='relu'),\n",
        "        L.Dense(5, activation='softmax'),\n",
        "        # ConditionalRandomField()\n",
        "    ], name='cnn')\n",
        "cnn.summary()\n",
        "x_in = Input(shape=(None,))\n",
        "x = cnn(x_in)\n",
        "out = CRF(x)\n",
        "model = Model(x_in, out)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"cnn\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 100)         516800    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 256)         77056     \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, None, 256)         196864    \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, None, 256)         196864    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, None, 5)           1285      \n",
            "=================================================================\n",
            "Total params: 988,869\n",
            "Trainable params: 988,869\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "cnn (Sequential)             (None, None, 5)           988869    \n",
            "_________________________________________________________________\n",
            "conditional_random_field (Co (None, None, 5)           25        \n",
            "=================================================================\n",
            "Total params: 988,894\n",
            "Trainable params: 988,894\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfPdoPTyGjv1"
      },
      "source": [
        "from typing import Union\n",
        "\n",
        "class CWS(ViterbiDecoder):\n",
        "    def parse_tag(self, text: Union[str, List[str]]):\n",
        "        \"\"\"text 是char级别的list或者str\"\"\"\n",
        "        encode_text = [tokenizer.token2id.get(w) for w in text]\n",
        "        nodes = model.predict([encode_text])[0]\n",
        "        labels = self.decode(nodes=nodes[:, :-1])\n",
        "        tags = [id2tag.get(i) for i in labels]\n",
        "        return tags\n",
        "\n",
        "    def cut(self, text):\n",
        "        tags = self.parse_tag(text)\n",
        "        assert len(tags) == len(text)\n",
        "        ans = ''\n",
        "        for word, tag in zip(text, tags):\n",
        "            ans += word\n",
        "            if tag in ['S', 'E']:\n",
        "                yield ans\n",
        "                ans = ''\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZDHhRCFGjv2",
        "outputId": "66941fd8-ab43-4b2c-c9ed-0e0a5a847378"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(tokenizer.char_data, tokenizer.label, test_size=0.02)\n",
        "print(len(x_train), len(x_test))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "85185 1739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXOfQ0UKGjv2"
      },
      "source": [
        "model.compile(loss=CRF.dense_loss, metrics=[CRF.dense_accuracy], optimizer='Adam')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epTGAAKjJaao"
      },
      "source": [
        "## 测试一条数据\n",
        "由于模型还未开始训练，因此输出的只是测试结果。\n",
        "创建维特比解码，模型没有训练，转移矩阵是一个随机数，解码肯定存在问题。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "AXiMNQLyGjv2",
        "outputId": "2b03e47e-6db9-4eec-b8d7-17960d2e82e9"
      },
      "source": [
        "ws = CWS(model.get_weights()[-1][:-1, :-1], starts=[0,3], ends=[2,3])\n",
        "\"/\".join(ws.cut('不知道这个玩意到底怎么样?'))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'不/知道/这个/玩意/到底/怎么/样?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl0DVdUiJxyx"
      },
      "source": [
        "## 创建模型回调\n",
        "模型的测试数据在每个epoch之后做一次评估。评估结果使用维特比解码。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FwvtLDHGjv2"
      },
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "import numpy as np\n",
        "\n",
        "class Evaluator(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        right = 0\n",
        "        total = 0\n",
        "        for sentence, tags in tqdm(zip(x_test, y_test)):\n",
        "            if len(sentence) == 0:\n",
        "                continue\n",
        "            y_pred = model.predict([sentence])[0] # [src, num_classes]\n",
        "            y_pred = ws.decode(y_pred[:, :-1])\n",
        "            \n",
        "            right += (y_pred == tags).sum()\n",
        "            total += len(tags)\n",
        "        print()\n",
        "        print(\"accuracy: \", right / total)\n",
        "        self.just_show()\n",
        "\n",
        "    @staticmethod\n",
        "    def just_show():\n",
        "        print(\"/\".join(ws.cut('我是中国人，我热爱中国！')))\n",
        "        print(\"/\".join(ws.cut('陕西师范大学位于陕西省西安市')))\n",
        "        print(\"/\".join(ws.cut('最近凑着热闹玩了玩全球人工智能技术创新大赛')))"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRB01ty_Gjv3",
        "outputId": "b0d0cf6e-20ce-440d-cd6b-4e5d34c09128"
      },
      "source": [
        "model.fit(DataLoader(x_train, y_train), \n",
        "          epochs=5, \n",
        "          callbacks=[Evaluator()])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1332/1332 [==============================] - ETA: 0s - loss: 8.7809 - dense_accuracy: 0.9864"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1739it [00:48, 35.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "accuracy:  0.9501109193985704\n",
            "我/是/中国人/，/我/热爱/中国/！\n",
            "陕西师范大学/位于/陕西省/西安市\n",
            "最近/凑/着/热闹/玩/了/玩/全球/人工/智能/技术/创新/大赛\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1332/1332 [==============================] - 459s 345ms/step - loss: 8.7809 - dense_accuracy: 0.9864\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/5\n",
            "1332/1332 [==============================] - ETA: 0s - loss: 8.4622 - dense_accuracy: 0.9874"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1739it [00:48, 35.66it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "accuracy:  0.9506655163914223\n",
            "我/是/中国人/，/我/热爱/中国/！\n",
            "陕西师范大学/位于/陕西省/西安市\n",
            "最近/凑/着/热闹/玩/了/玩/全球/人工/智能/技术/创新/大赛\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1332/1332 [==============================] - 459s 345ms/step - loss: 8.4622 - dense_accuracy: 0.9874\n",
            "Epoch 3/5\n",
            "1332/1332 [==============================] - ETA: 0s - loss: 8.2498 - dense_accuracy: 0.9881"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1739it [00:48, 35.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "accuracy:  0.9531180675375893\n",
            "我/是/中国人/，/我/热爱/中国/！\n",
            "陕西师范大学/位于/陕西省/西安市\n",
            "最近/凑/着/热闹/玩/了/玩/全球/人工/智能/技术/创新/大赛\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1332/1332 [==============================] - 457s 343ms/step - loss: 8.2498 - dense_accuracy: 0.9881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/5\n",
            "1332/1332 [==============================] - ETA: 0s - loss: 8.0876 - dense_accuracy: 0.9888"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1739it [00:48, 35.68it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "accuracy:  0.9540916933694849\n",
            "我/是/中国人/，/我/热爱/中国/！\n",
            "陕西师范大学/位于/陕西省/西安市\n",
            "最近/凑/着/热闹/玩/了/玩/全球/人工/智能/技术/创新/大赛\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1332/1332 [==============================] - 457s 343ms/step - loss: 8.0876 - dense_accuracy: 0.9888\n",
            "Epoch 5/5\n",
            "1332/1332 [==============================] - ETA: 0s - loss: 7.9685 - dense_accuracy: 0.9893"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1739it [00:49, 35.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "accuracy:  0.9550776435789993\n",
            "我/是/中国人/，/我/热爱/中国/！\n",
            "陕西师范大学/位于/陕西省/西安市\n",
            "最近/凑/着/热闹/玩/了/玩/全球/人工/智能/技术/创新/大赛\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1332/1332 [==============================] - 457s 343ms/step - loss: 7.9685 - dense_accuracy: 0.9893\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9f3933a350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRb0fd0UnLvN",
        "outputId": "f7890a2d-bd8f-47e0-a445-219e912a5b67"
      },
      "source": [
        "model.save('/content/drive/MyDrive/data/icwb2-data/cnn-cws')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/data/icwb2-data/cnn-cws/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HwV2kxw3NHt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
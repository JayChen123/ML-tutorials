{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 完成实体的关系分类模型\n",
    "[文章地址](http://blackedu.vip/2021/04/关系抽取模型初步/)\n",
    "\n",
    "关系分类采用Bert，训练数据使用[百度公开比赛数据](https://aistudio.baidu.com/aistudio/competition/detail/46)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = 'data/DuIE_2_0/train.json'\n",
    "dev_json = 'data/DuIE_2_0/dev.json'\n",
    "test_json = 'data/DuIE_2_0/test.json'\n",
    "schema_json = 'data/DuIE_2_0/schema.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def read_json_file(json_name):\n",
    "    data = []\n",
    "    with open(json_name) as reader:\n",
    "        lines = reader.read().split('\\n')\n",
    "        for line in tqdm(lines[:-1], desc='read ...'): # 去除最后的空格\n",
    "            data.append(json.loads(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read ...: 100%|██████████| 48/48 [00:00<00:00, 207126.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# {'object_type': {'@value': '学校'}, 'predicate': '毕业院校', 'subject_type': '人物'},\n",
    "#  {'object_type': {'@value': '人物'}, 'predicate': '嘉宾', 'subject_type': '电视综艺'},\n",
    "#  {'object_type': {'inWork': '影视作品', '@value': '人物'},\n",
    "#   'predicate': '配音', 'subject_type': '娱乐人物'}\n",
    "\n",
    "schema = read_json_file(schema_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = [item['predicate'] for item in schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['毕业院校', '嘉宾', '配音', '主题曲', '代言人']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2id = {val:idx for idx, val in enumerate(relations, 1)}\n",
    "id2rel = {v:k for k, v in rel2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert config\n",
    "import os\n",
    "\n",
    "model_path = 'bert_models/chinese_L-12_H-768_A-12/'\n",
    "\n",
    "bert_config = os.path.join(model_path, 'bert_config.json')\n",
    "check_point = os.path.join(model_path, 'bert_model.ckpt')\n",
    "vocab = os.path.join(model_path, 'vocab.txt')\n",
    "model_type = 'bert'\n",
    "\n",
    "export_model_name = 'weights/rel-bert-base-best.weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from bert4keras.tokenizers import Tokenizer, load_vocab\n",
    "from bert4keras.models import build_transformer_model\n",
    "\n",
    "token_dict = load_vocab(vocab)\n",
    "tokenizer = Tokenizer(token_dict=token_dict, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read ...: 100%|██████████| 171293/171293 [00:02<00:00, 84004.11it/s]\n",
      "read ...: 100%|██████████| 20674/20674 [00:00<00:00, 142845.44it/s]\n"
     ]
    }
   ],
   "source": [
    "train = read_json_file(train_json)\n",
    "dev = read_json_file(dev_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '《邪少兵王》是冰火未央写的网络小说连载于旗峰天下',\n",
       "  'spo_list': [{'predicate': '作者',\n",
       "    'object_type': {'@value': '人物'},\n",
       "    'subject_type': '图书作品',\n",
       "    'object': {'@value': '冰火未央'},\n",
       "    'subject': '邪少兵王'}]},\n",
       " {'text': 'GV-971由中国海洋大学、中国科学院上海药物研究所（下称“上海药物所”）和上海绿谷制药有限公司（下称“绿谷制药”）联合研发，不同于传统靶向抗体药物，GV-971是从海藻中提取的海洋寡糖类分子',\n",
       "  'spo_list': [{'predicate': '简称',\n",
       "    'object_type': {'@value': 'Text'},\n",
       "    'subject_type': '机构',\n",
       "    'object': {'@value': '上海药物所'},\n",
       "    'subject': '中国科学院上海药物研究所'}]}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id('[SEP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence, to_categorical\n",
    "from bert4keras.snippets import sequence_padding, to_array, DataGenerator\n",
    "\n",
    "MAXLEN = 200\n",
    "\n",
    "class DataLoader(DataGenerator):\n",
    "    def __init__(self, data, batch_size=32):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def read_data(self):\n",
    "        for item in self.data:\n",
    "            text = item['text']\n",
    "            for spo in item['spo_list']:\n",
    "                yield text, spo['predicate'], spo['subject'], spo['object']['@value']\n",
    "    \n",
    "    def __iter__(self, random=False):\n",
    "        batch_token, batch_segment, batch_label = [], [], []\n",
    "        for text, p, s, o in self.read_data():\n",
    "            token_id, segment_id = tokenizer.encode(first_text=text, second_text=s, maxlen=MAXLEN)\n",
    "            obj_token_id = tokenizer.tokens_to_ids(o)\n",
    "            token_id = token_id + obj_token_id + [102]\n",
    "            segment_id = segment_id + [1]*(len(obj_token_id)+1)\n",
    "            \n",
    "            batch_token.append(token_id)\n",
    "            batch_segment.append(segment_id)\n",
    "            batch_label.append([rel2id.get(p, 0)])\n",
    "            \n",
    "            if len(batch_label) == self.batch_size:\n",
    "                batch_token = sequence_padding(batch_token)\n",
    "                batch_segment = sequence_padding(batch_segment)\n",
    "                yield [batch_token, batch_segment], to_array(batch_label)\n",
    "                batch_token, batch_segment, batch_label = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 210)\n",
      "(32, 210)\n",
      "(32, 1)\n",
      "(32, 191)\n",
      "(32, 191)\n",
      "(32, 1)\n",
      "(32, 172)\n",
      "(32, 172)\n",
      "(32, 1)\n",
      "(32, 189)\n",
      "(32, 189)\n",
      "(32, 1)\n",
      "(32, 175)\n",
      "(32, 175)\n",
      "(32, 1)\n",
      "(32, 206)\n",
      "(32, 206)\n",
      "(32, 1)\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(DataLoader(dev)):\n",
    "    (batch_token, batch_segment), batch_label = batch\n",
    "    print(batch_token.shape)\n",
    "    print(batch_segment.shape)\n",
    "    print(batch_label.shape)\n",
    "    \n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = build_transformer_model(bert_config, checkpoint_path=check_point, model=model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# output_layer = 'Transformer-11-FeedForward-Norm'\n",
    "# output = bert.get_layer(output_layer)\n",
    "output = Lambda(lambda x: x[:, 0], name='CLS')(bert.output)\n",
    "output = Dense(len(rel2id)+1, activation='softmax')(output)\n",
    "model = Model(bert.input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (Embedding)     (None, None, 768)    16226304    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          \n",
      "                                                                 Embedding-Dropout[0][0]          \n",
      "                                                                 Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          \n",
      "                                                                 Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
      "                                                                 Transformer-0-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
      "                                                                 Transformer-1-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
      "                                                                 Transformer-2-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]\n",
      "                                                                 Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
      "                                                                 Transformer-3-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]\n",
      "                                                                 Transformer-3-FeedForward-Norm[0]\n",
      "                                                                 Transformer-3-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]\n",
      "                                                                 Transformer-4-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
      "                                                                 Transformer-4-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]\n",
      "                                                                 Transformer-4-FeedForward-Norm[0]\n",
      "                                                                 Transformer-4-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]\n",
      "                                                                 Transformer-5-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
      "                                                                 Transformer-5-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]\n",
      "                                                                 Transformer-5-FeedForward-Norm[0]\n",
      "                                                                 Transformer-5-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]\n",
      "                                                                 Transformer-6-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
      "                                                                 Transformer-6-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]\n",
      "                                                                 Transformer-6-FeedForward-Norm[0]\n",
      "                                                                 Transformer-6-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]\n",
      "                                                                 Transformer-7-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
      "                                                                 Transformer-7-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]\n",
      "                                                                 Transformer-7-FeedForward-Norm[0]\n",
      "                                                                 Transformer-7-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]\n",
      "                                                                 Transformer-8-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
      "                                                                 Transformer-8-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]\n",
      "                                                                 Transformer-8-FeedForward-Norm[0]\n",
      "                                                                 Transformer-8-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]\n",
      "                                                                 Transformer-9-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
      "                                                                 Transformer-9-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]\n",
      "                                                                 Transformer-9-FeedForward-Norm[0]\n",
      "                                                                 Transformer-9-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]\n",
      "                                                                 Transformer-10-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
      "                                                                 Transformer-10-FeedForward-Dropou\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0\n",
      "                                                                 Transformer-10-FeedForward-Norm[0\n",
      "                                                                 Transformer-10-FeedForward-Norm[0\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0\n",
      "                                                                 Transformer-11-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
      "                                                                 Transformer-11-FeedForward-Dropou\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]\n",
      "__________________________________________________________________________________________________\n",
      "CLS (Lambda)                    (None, 768)          0           Transformer-11-FeedForward-Norm[0\n",
      "__________________________________________________________________________________________________\n",
      "dense_73 (Dense)                (None, 49)           37681       CLS[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 101,714,737\n",
      "Trainable params: 101,714,737\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.metrics import accuracy, Recall\n",
    "\n",
    "model.compile(loss=SparseCategoricalCrossentropy(), optimizer=Adam(1e-5), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "def evalute_relation(text, subject, obj):\n",
    "    token_id, segment_id = tokenizer.encode(first_text=text, second_text=subject)\n",
    "    obj_id = tokenizer.tokens_to_ids(obj)\n",
    "    token_id = to_array([token_id + obj_id + [102]])\n",
    "    segment_id = to_array([segment_id + [1] * (len(obj_id) + 1)])\n",
    "    y_pred = model.predict([token_id, segment_id])\n",
    "    return id2rel.get(y_pred.argmax(), '未知')\n",
    "\n",
    "\n",
    "class EvalCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.loss_lower = 1e5\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs['val_loss'] < self.loss_lower:\n",
    "            self.loss_lower = logs['val_loss']\n",
    "            \n",
    "            model.save_weights(export_model_name)\n",
    "        \n",
    "        self.just_show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def just_show():\n",
    "        data = [{'text': '根据启信宝的数据显示，泡泡玛特主体公司北京泡泡玛特文化创意 \\\n",
    "                有限公司，法定代表人为王宁，该公司曾于2017年2月登陆新三板，在2019年4月终止挂牌',\n",
    "               'subject': '北京泡泡玛特文化创意',\n",
    "               'obj': '王宁'},\n",
    "                {'text': \"《明早起飞》是由明太鱼作词，满江作曲，戴娆演唱的一首歌曲\",\n",
    "                'obj': '满江',\n",
    "                'subject': '明早起飞'},\n",
    "                {'text': \"《明早起飞》是由明太鱼作词，满江作曲，戴娆演唱的一首歌曲\",\n",
    "                'subject': '明早起飞',\n",
    "                'obj': '明太鱼'},\n",
    "                {'text': \"【#真正男子汉#新兵欧豪报到】男子汉，我一直在渴望 我不怕什么挑战，就要证明给你们看 穿上这身军装，我就是军人\",\n",
    "                'subject': '真正男子汉',\n",
    "                'obj': '欧豪'}\n",
    "               ]\n",
    "        for each in data:\n",
    "            print(evalute_relation(**each))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinct/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 383s 383ms/step - loss: 0.8566 - accuracy: 0.7358 - val_loss: 0.4732 - val_accuracy: 0.8649\n",
      "董事长\n",
      "作曲\n",
      "作词\n",
      "嘉宾\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 381s 381ms/step - loss: 0.3608 - accuracy: 0.8702 - val_loss: 0.5939 - val_accuracy: 0.8913\n",
      "董事长\n",
      "作曲\n",
      "作词\n",
      "嘉宾\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 381s 381ms/step - loss: 0.3076 - accuracy: 0.8921 - val_loss: 0.0930 - val_accuracy: 0.9196\n",
      "董事长\n",
      "作曲\n",
      "作词\n",
      "嘉宾\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 381s 381ms/step - loss: 0.2626 - accuracy: 0.9151 - val_loss: 0.3804 - val_accuracy: 0.9227\n",
      "董事长\n",
      "作曲\n",
      "作词\n",
      "嘉宾\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 380s 380ms/step - loss: 0.2372 - accuracy: 0.9208 - val_loss: 0.5510 - val_accuracy: 0.9315\n",
      "董事长\n",
      "作曲\n",
      "作词\n",
      "嘉宾\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 381s 381ms/step - loss: 0.2202 - accuracy: 0.9269 - val_loss: 0.0118 - val_accuracy: 0.9336\n",
      "董事长\n",
      "作曲\n",
      "作词\n",
      "嘉宾\n",
      "Epoch 7/30\n",
      " 766/1000 [=====================>........] - ETA: 1:07 - loss: 0.2306 - accuracy: 0.9257"
     ]
    }
   ],
   "source": [
    "# for batch in DataLoader(dev, batch_size=16):\n",
    "#     (batch_token, batch_segment), batch_label = batch\n",
    "#     model.fit(x=[batch_token, batch_segment], y=batch_label, epochs=1)\n",
    "train_iter = DataLoader(train, batch_size=16)\n",
    "dev_iter = DataLoader(dev, batch_size=16)\n",
    "\n",
    "if os.path.exists(export_model_name):\n",
    "    model.load_weights(export_model_name)\n",
    "    print('==== load model weight!!! =====')\n",
    "    \n",
    "model.fit(train_iter.forfit(), \n",
    "          validation_data=dev_iter.forfit(), \n",
    "          epochs=30, \n",
    "          steps_per_epoch=1000, \n",
    "          validation_steps=1000, callbacks=[EvalCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
